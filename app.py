import streamlit as st
import time
import google.generativeai as genai
import asyncio
import edge_tts
import speech_recognition as sr
from streamlit_mic_recorder import mic_recorder
from io import BytesIO
import re
from datetime import datetime
import pytz # Ù…ÙƒØªØ¨Ø© Ø§Ù„Ù…Ù†Ø§Ø·Ù‚ Ø§Ù„Ø²Ù…Ù†ÙŠØ©

# ==========================================
# ğŸ›ï¸ Ù„ÙˆØ­Ø© ØªØ­ÙƒÙ… Ø§Ù„Ù…Ø¹Ù„Ù… (Ø¹Ø¯Ù„ Ù‡Ù†Ø§ ÙŠÙˆÙ…ÙŠØ§Ù‹)
# ==========================================

# 1. ÙƒÙˆØ¯ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ù„Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ…
DAILY_PASSWORD = "SCIENCE_DAY1" 

# 2. ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† (Ø§Ù„Ø³Ù†Ø©-Ø§Ù„Ø´Ù‡Ø±-Ø§Ù„ÙŠÙˆÙ…)
EXAM_DATE = "2024-05-20" 

# 3. ÙˆÙ‚Øª Ø§Ù„Ø¨Ø¯Ø§ÙŠØ© ÙˆØ§Ù„Ù†Ù‡Ø§ÙŠØ© (Ø¨Ù†Ø§Ø¸Ù… 24 Ø³Ø§Ø¹Ø©)
# Ù…Ø«Ø§Ù„: Ù…Ù† 1 Ø¸Ù‡Ø±Ù‹Ø§ (13) Ø¥Ù„Ù‰ 2 Ø¸Ù‡Ø±Ù‹Ø§ (14)
START_HOUR = 13 
END_HOUR = 14   

# 4. ØªÙˆÙ‚ÙŠØªÙƒ Ø§Ù„Ù…Ø­Ù„ÙŠ (Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ Ù„Ø¶Ø¨Ø· Ø§Ù„Ø³Ø§Ø¹Ø©)
# Ù„Ù…ØµØ±: 'Africa/Cairo' | Ù„Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©: 'Asia/Riyadh'
MY_TIMEZONE = 'Africa/Cairo' 

# ==========================================

st.set_page_config(page_title="Ø§Ù„Ø§Ø®ØªØ¨Ø§Ø± Ø§Ù„Ù…Ø­Ø¯Ø¯ Ø¨ÙˆÙ‚Øª", page_icon="â³", layout="centered")

# --- Ø¯Ø§Ù„Ø© Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙˆÙ‚Øª (Ø§Ù„Ø­Ø§Ø±Ø³ Ø§Ù„Ø°ÙƒÙŠ) ---
def check_time_window():
    # Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ø­Ø§Ù„ÙŠ Ø¨ØªÙˆÙ‚ÙŠØª Ø¨Ù„Ø¯Ùƒ
    tz = pytz.timezone(MY_TIMEZONE)
    now = datetime.now(tz)
    
    current_date = now.strftime("%Y-%m-%d")
    current_hour = now.hour
    current_minute = now.minute
    
    # 1. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„ÙŠÙˆÙ…
    if current_date != EXAM_DATE:
        return False, f"â›” Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† Ù„ÙŠØ³ Ø§Ù„ÙŠÙˆÙ…. ØªØ§Ø±ÙŠØ® Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† Ø§Ù„Ù…Ù‚Ø±Ø±: {EXAM_DATE}"
    
    # 2. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø³Ø§Ø¹Ø© (Ù‚Ø¨Ù„ Ø§Ù„Ù…ÙˆØ¹Ø¯)
    if current_hour < START_HOUR:
        return False, f"â³ Ù„Ù… ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† Ø¨Ø¹Ø¯. ÙŠØ¨Ø¯Ø£ Ø§Ù„Ø³Ø§Ø¹Ø© {START_HOUR}:00 Ø¨ØªÙˆÙ‚ÙŠØª {MY_TIMEZONE}"
    
    # 3. Ø§Ù„ØªØ­Ù‚Ù‚ Ù…Ù† Ø§Ù„Ø³Ø§Ø¹Ø© (Ø¨Ø¹Ø¯ Ø§Ù„Ù…ÙˆØ¹Ø¯)
    # Ø§Ù„Ù…Ø³Ù…ÙˆØ­: Ø£Ù† ØªÙƒÙˆÙ† Ø§Ù„Ø³Ø§Ø¹Ø© Ø£ÙƒØ¨Ø± Ù…Ù† Ø£Ùˆ ØªØ³Ø§ÙˆÙŠ Ø§Ù„Ø¨Ø¯Ø§ÙŠØ©ØŒ ÙˆØ£Ù‚Ù„ ØªÙ…Ø§Ù…Ø§Ù‹ Ù…Ù† Ø§Ù„Ù†Ù‡Ø§ÙŠØ©
    # Ù…Ø«Ø§Ù„: Ù…Ù† 13:00 Ø­ØªÙ‰ 13:59 (Ø¨Ù…Ø¬Ø±Ø¯ Ø£Ù† ØªØ£ØªÙŠ 14:00 ÙŠØºÙ„Ù‚)
    if current_hour >= END_HOUR:
        return False, "ğŸ›‘ Ø§Ù†ØªÙ‡Ù‰ ÙˆÙ‚Øª Ø§Ù„Ø§Ù…ØªØ­Ø§Ù†! ØªÙ… Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù… ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹."
        
    # Ø­Ø³Ø§Ø¨ Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© Ù„Ù„Ø¥ØºÙ„Ø§Ù‚
    # ÙˆÙ‚Øª Ø§Ù„Ù†Ù‡Ø§ÙŠØ© Ù‡Ùˆ END_HOUR:00
    # Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠØ© = (Ø³Ø§Ø¹Ø© Ø§Ù„Ù†Ù‡Ø§ÙŠØ© * 60) - (Ø§Ù„Ø³Ø§Ø¹Ø© Ø§Ù„Ø­Ø§Ù„ÙŠØ© * 60 + Ø§Ù„Ø¯Ù‚Ø§Ø¦Ù‚ Ø§Ù„Ø­Ø§Ù„ÙŠØ©)
    end_minutes = END_HOUR * 60
    current_total_minutes = current_hour * 60 + current_minute
    remaining = end_minutes - current_total_minutes
    
    return True, remaining

# ==========================================
# ØªÙ†ÙÙŠØ° Ø§Ù„ØªØ­Ù‚Ù‚ (Ù‚Ø¨Ù„ ØªØ´ØºÙŠÙ„ Ø£ÙŠ ÙƒÙˆØ¯ Ø¢Ø®Ø±)
is_open, message = check_time_window()

if not is_open:
    st.error(message)
    st.image("https://cdn-icons-png.flaticon.com/512/483/483696.png", width=150)
    st.stop() # ÙŠÙ‚ØªÙ„ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ù‡Ù†Ø§ØŒ Ù„Ù† ÙŠØ¸Ù‡Ø± Ø£ÙŠ Ø´ÙŠØ¡ Ø¨Ø§Ù„Ø£Ø³ÙÙ„
# ==========================================


# --- Ø¨Ø§Ù‚ÙŠ ÙƒÙˆØ¯ Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ (Ù„Ø§ ÙŠØ¹Ù…Ù„ Ø¥Ù„Ø§ Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„ÙˆÙ‚Øª ØµØ­ÙŠØ­Ø§Ù‹) ---

# ... Ø¯ÙˆØ§Ù„ Ø§Ù„ØµÙˆØª ÙˆØ§Ù„Ø°ÙƒØ§Ø¡ Ø§Ù„Ø§ØµØ·Ù†Ø§Ø¹ÙŠ Ø§Ù„Ù…Ø¹ØªØ§Ø¯Ø© ...
def prepare_text(text):
    text = re.sub(r'[\*\#\-\_]', '', text)
    return text

async def generate_speech(text, output_file, voice_code):
    clean_text = prepare_text(text)
    communicate = edge_tts.Communicate(clean_text, voice_code, rate="-5%")
    await communicate.save(output_file)

def speech_to_text(audio_bytes):
    r = sr.Recognizer()
    try:
        audio_file = sr.AudioFile(BytesIO(audio_bytes))
        with audio_file as source:
            r.adjust_for_ambient_noise(source)
            audio_data = r.record(source)
            text = r.recognize_google(audio_data, language="ar-EG")
            return text
    except:
        return None

try:
    if "GOOGLE_API_KEY" in st.secrets:
        genai.configure(api_key=st.secrets["GOOGLE_API_KEY"])
    else:
        st.error("Ù…ÙØªØ§Ø­ Ø¬ÙˆØ¬Ù„ Ù…ÙÙ‚ÙˆØ¯"); st.stop()
    
    all_models = genai.list_models()
    my_models = [m.name for m in all_models if 'generateContent' in m.supported_generation_methods]
    active_model = next((m for m in my_models if 'flash' in m), my_models[0])
    model = genai.GenerativeModel(active_model)
except:
    st.error("Ø®Ø·Ø£ ØªÙ‚Ù†ÙŠ"); st.stop()


# ===== ÙˆØ§Ø¬Ù‡Ø© ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ Ø§Ù„ÙŠÙˆÙ…ÙŠØ© =====
if "logged_in" not in st.session_state:
    st.session_state.logged_in = False

if not st.session_state.logged_in:
    st.title("ğŸ” Ø§Ù…ØªØ­Ø§Ù† Ø§Ù„Ø¹Ù„ÙˆÙ… Ø§Ù„ÙŠÙˆÙ…ÙŠ")
    st.caption(f"Ù…ØªØ§Ø­ Ø§Ù„ÙŠÙˆÙ… ÙÙ‚Ø· Ù…Ù† {START_HOUR}:00 Ø¥Ù„Ù‰ {END_HOUR}:00")
    
    password = st.text_input("Ø£Ø¯Ø®Ù„ ÙƒÙˆØ¯ Ø§Ù„ÙŠÙˆÙ…:", type="password")
    if st.button("Ø¯Ø®ÙˆÙ„"):
        if password == DAILY_PASSWORD:
            st.session_state.logged_in = True
            st.rerun()
        else:
            st.error("Ø§Ù„ÙƒÙˆØ¯ ØºÙŠØ± ØµØ­ÙŠØ­ Ù„Ù‡Ø°Ø§ Ø§Ù„ÙŠÙˆÙ….")
    st.stop()

# ===== ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„Ø§Ù…ØªØ­Ø§Ù† (Ø¨Ø¹Ø¯ Ø§Ù„Ø¯Ø®ÙˆÙ„) =====

# Ø¹Ø±Ø¶ Ø§Ù„Ø¹Ø¯Ø§Ø¯ Ø§Ù„Ù…ØªØ¨Ù‚ÙŠ Ù„Ù„Ø¥ØºÙ„Ø§Ù‚ Ø§Ù„Ø¹Ø§Ù…
is_still_open, remaining_mins = check_time_window()
if not is_still_open:
    st.session_state.logged_in = False
    st.rerun()

st.sidebar.title(f"â³ Ø¨Ø§Ù‚ÙŠ: {remaining_mins} Ø¯Ù‚ÙŠÙ‚Ø©")
st.sidebar.warning(f"Ø³ÙŠØºÙ„Ù‚ Ø§Ù„Ù†Ø¸Ø§Ù… ØªÙ…Ø§Ù…Ø§Ù‹ Ø§Ù„Ø³Ø§Ø¹Ø© {END_HOUR}:00")

st.title("ğŸ™ï¸ Ø§Ù„Ù…Ø¹Ù„Ù… Ø§Ù„Ø°ÙƒÙŠ (Ø§Ø®ØªØ¨Ø§Ø±)")

# Ø®ÙŠØ§Ø±Ø§Øª Ø§Ù„ØµÙˆØª
voice_options = {
    "ğŸ‡ªğŸ‡¬ Ù…Ø³ØªØ± Ø´Ø§ÙƒØ±": "ar-EG-ShakirNeural",
    "ğŸ‡ªğŸ‡¬ Ù…Ø³ Ø³Ù„Ù…Ù‰": "ar-EG-SalmaNeural"
}
selected_voice_code = voice_options["ğŸ‡ªğŸ‡¬ Ù…Ø³ØªØ± Ø´Ø§ÙƒØ±"] 

st.markdown("---")
st.write("Ø§Ø¶ØºØ· ÙˆØªØ­Ø¯Ø«:")

audio_input = mic_recorder(
    start_prompt="ğŸ¤ ØªØ­Ø¯Ø«",
    stop_prompt="â¹ï¸ Ø¥Ø±Ø³Ø§Ù„",
    key='recorder',
    format="wav"
)

if audio_input:
    with st.spinner("ğŸ‘‚ ..."):
        user_text = speech_to_text(audio_input['bytes'])
    
    if user_text:
        st.success(f"ğŸ—£ï¸: {user_text}")
        with st.spinner("ğŸ§  ..."):
            try:
                prompt = f"""
                Ø£Ù†Øª Ù…Ø¹Ù„Ù… Ù…ØµØ±ÙŠ. Ø§Ù„Ø·Ø§Ù„Ø¨ ÙŠØ³Ø£Ù„Ùƒ Ø£Ùˆ ÙŠØ¬ÙŠØ¨Ùƒ: '{user_text}'.
                Ø±Ø¯ Ø¹Ù„ÙŠÙ‡ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ© ÙˆØ¨Ø¥ÙŠØ¬Ø§Ø² Ø´Ø¯ÙŠØ¯.
                """
                response = model.generate_content(prompt)
                st.markdown(f"### ğŸ“˜ Ø§Ù„Ø±Ø¯:\n{response.text}")
                
                output_file = "response.mp3"
                asyncio.run(generate_speech(response.text, output_file, selected_voice_code))
                st.audio(output_file, format='audio/mp3', autoplay=True)
                
            except Exception as e:
                st.error(f"Ø®Ø·Ø£: {e}")
