import streamlit as st
import time
import google.generativeai as genai
import asyncio
import edge_tts
import speech_recognition as sr
from streamlit_mic_recorder import mic_recorder
from io import BytesIO

# ===== 1. Ø¥Ø¹Ø¯Ø§Ø¯ Ø§Ù„ØµÙØ­Ø© ÙˆØ§Ù„Ø³ØªØ§ÙŠÙ„ =====
st.set_page_config(page_title="Ø§Ù„Ù…Ø¹Ù„Ù… Ø§Ù„ØµÙˆØªÙŠ", page_icon="ğŸ™ï¸", layout="centered")

# --- Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ù†Øµ Ø¥Ù„Ù‰ ØµÙˆØª (Ø§Ù„Ù…Ø¹Ù„Ù… ÙŠØªØ­Ø¯Ø«) ---
async def generate_speech(text, output_file):
    communicate = edge_tts.Communicate(text, "ar-EG-ShakirNeural")
    await communicate.save(output_file)

# --- Ø¯Ø§Ù„Ø© ØªØ­ÙˆÙŠÙ„ ØµÙˆØª Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¥Ù„Ù‰ Ù†Øµ ---
def speech_to_text(audio_bytes):
    r = sr.Recognizer()
    try:
        # ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø®Ø§Ù… Ø¥Ù„Ù‰ Ù…Ù„Ù ØµÙˆØªÙŠ ÙÙŠ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
        audio_file = sr.AudioFile(BytesIO(audio_bytes))
        with audio_file as source:
            audio_data = r.record(source)
            # Ø§Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù… (Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ù…ØµØ±ÙŠØ©/Ø§Ù„Ø¹Ø±Ø¨ÙŠØ©)
            text = r.recognize_google(audio_data, language="ar-EG")
            return text
    except sr.UnknownValueError:
        return None
    except Exception as e:
        return f"Error: {e}"

# --- Ø¥Ø¹Ø¯Ø§Ø¯ Ø§ØªØµØ§Ù„ Ø¬ÙˆØ¬Ù„ (Ø§Ù„Ø°ÙƒÙŠ) ---
active_model_name = None
try:
    api_key = st.secrets["GOOGLE_API_KEY"]
    genai.configure(api_key=api_key)
    
    available_models = [m.name for m in genai.list_models() if 'generateContent' in m.supported_generation_methods]
    if available_models:
        priority = [m for m in available_models if 'flash' in m] + [m for m in available_models if 'pro' in m]
        active_model_name = priority[0] if priority else available_models[0]
        
        # Ø´Ø®ØµÙŠØ© Ø§Ù„Ù…Ø¹Ù„Ù…
        system_instruction = """
        Ø£Ù†Øª Ù…Ø¹Ù„Ù… Ø¹Ù„ÙˆÙ… ØµÙˆØªÙŠ Ø§Ø³Ù…Ù‡ 'Ù…Ø³ØªØ± Ø´Ø§ÙƒØ±'.
        Ø£Ø³Ù„ÙˆØ¨Ùƒ: ØµÙˆØªÙŠØŒ Ø¹ÙÙˆÙŠØŒ Ù…Ø±Ø­ØŒ Ø¨Ø§Ù„Ù„Ù‡Ø¬Ø© Ø§Ù„Ø¨ÙŠØ¶Ø§Ø¡ Ø§Ù„Ù…Ø¨Ø³Ø·Ø© Ø£Ùˆ Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„Ø³Ù‡Ù„Ø©.
        Ù„Ø§ ØªØ³ØªØ®Ø¯Ù… ØªÙ†Ø³ÙŠÙ‚Ø§Øª Ù…Ø¹Ù‚Ø¯Ø© (Ù…Ø«Ù„ Ø§Ù„Ø¬Ø¯Ø§ÙˆÙ„) Ù„Ø£Ù†Ùƒ ØªØªØ­Ø¯Ø« ØµÙˆØªÙŠØ§Ù‹.
        Ø§Ø¬Ø¹Ù„ Ø¥Ø¬Ø§Ø¨Ø§ØªÙƒ Ù‚ØµÙŠØ±Ø© (ÙÙ‚Ø±Ø© ÙˆØ§Ø­Ø¯Ø© Ø£Ùˆ ÙÙ‚Ø±ØªÙŠÙ†) Ø­ØªÙ‰ Ù„Ø§ ÙŠÙ…Ù„ Ø§Ù„Ø·Ø§Ù„Ø¨ Ù…Ù† Ø§Ù„Ø§Ø³ØªÙ…Ø§Ø¹.
        Ø±Ø­Ø¨ Ø¨Ø§Ù„Ø·Ø§Ù„Ø¨ Ø¹Ù†Ø¯ Ø§Ù„Ø­Ø§Ø¬Ø© ÙˆØ´Ø¬Ø¹Ù‡.
        """
        model = genai.GenerativeModel(active_model_name, system_instruction=system_instruction)
    else:
        st.error("âš ï¸ Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…ÙˆØ¯ÙŠÙ„Ø§Øª Ù…ØªØ§Ø­Ø©"); st.stop()
except:
    st.error("âš ï¸ Ø®Ø·Ø£ ÙÙŠ Ø§Ù„Ø§ØªØµØ§Ù„"); st.stop()

# ===== 2. ÙˆØ§Ø¬Ù‡Ø© Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ =====
st.title("ğŸ™ï¸ Ø§Ù„Ù…Ø¹Ù„Ù… Ø§Ù„Ø°ÙƒÙŠ (Ù…Ø­Ø§Ø¯Ø«Ø© ØµÙˆØªÙŠØ©)")

# ===== 3. ØªØ³Ø¬ÙŠÙ„ Ø§Ù„Ø¯Ø®ÙˆÙ„ =====
if "logged_in" not in st.session_state:
    password = st.text_input("ğŸ”‘ ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ±", type="password")
    if password == "SCIENCE60":
        st.session_state.logged_in = True
        st.rerun()
    elif password:
        st.warning("ÙƒÙ„Ù…Ø© Ø§Ù„Ù…Ø±ÙˆØ± Ø®Ø·Ø£")
    st.stop()

# ===== 4. Ø§Ù„Ø¹Ø¯Ø§Ø¯ =====
if "start_time" not in st.session_state: st.session_state.start_time = time.time()
remaining = 3600 - (time.time() - st.session_state.start_time)
if remaining <= 0: st.error("Ø§Ù†ØªÙ‡Ù‰ Ø§Ù„ÙˆÙ‚Øª"); st.stop()
st.info(f"â³ Ø¨Ø§Ù‚ÙŠ: {int(remaining//60)} Ø¯Ù‚ÙŠÙ‚Ø©")

# ===== 5. Ù…Ù†Ø·Ù‚Ø© Ø§Ù„Ù…Ø­Ø§Ø¯Ø«Ø© Ø§Ù„ØµÙˆØªÙŠØ© =====
st.markdown("---")
st.subheader("ØªØ­Ø¯Ø« Ù…Ø¹ Ø§Ù„Ù…Ø¹Ù„Ù… Ù…Ø¨Ø§Ø´Ø±Ø© ğŸ‘‡")

# Ø¹Ù…ÙˆØ¯ÙŠÙ†: ÙˆØ§Ø­Ø¯ Ù„Ù„Ø²Ø± ÙˆÙˆØ§Ø­Ø¯ Ù„Ø¹Ø±Ø¶ Ø§Ù„Ø­Ø§Ù„Ø©
col1, col2 = st.columns([1, 3])

with col1:
    st.write("Ø§Ø¶ØºØ· Ù„Ù„ØªØ­Ø¯Ø«:")
    # Ø²Ø± Ø§Ù„ØªØ³Ø¬ÙŠÙ„ (ÙŠØ¹ÙŠØ¯ Ø¨Ø§ÙŠØªØ§Øª Ø§Ù„ØµÙˆØª)
    audio_input = mic_recorder(
        start_prompt="ğŸ¤ Ø§Ø¶ØºØ· ÙˆØ³Ø¬Ù‘Ù„",
        stop_prompt="â¹ï¸ Ø¥Ù†Ù‡Ø§Ø¡ ÙˆØ¥Ø±Ø³Ø§Ù„",
        key='recorder',
        format="wav" # Ù…Ù‡Ù… Ø¬Ø¯Ø§Ù‹ Ù„Ù„ØªØ¹Ø±Ù Ø¹Ù„Ù‰ Ø§Ù„ÙƒÙ„Ø§Ù…
    )

user_text = ""

# Ù…Ù†Ø·Ù‚ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
if audio_input:
    with st.spinner("ğŸ§ Ø£Ø³ØªÙ…Ø¹ Ø¥Ù„ÙŠÙƒ..."):
        # 1. ØªØ­ÙˆÙŠÙ„ ØµÙˆØª Ø§Ù„Ø·Ø§Ù„Ø¨ Ù„Ù†Øµ
        transcribed_text = speech_to_text(audio_input['bytes'])
        
        if transcribed_text:
            user_text = transcribed_text
            st.success(f"ğŸ—£ï¸ Ø£Ù†Øª Ù‚Ù„Øª: {user_text}")
        else:
            st.warning("âš ï¸ Ù„Ù… Ø£Ø³Ù…Ø¹ ØµÙˆØªÙƒ Ø¨ÙˆØ¶ÙˆØ­ØŒ Ø­Ø§ÙˆÙ„ Ù…Ø±Ø© Ø£Ø®Ø±Ù‰.")

# Ø¥Ø°Ø§ ÙƒØ§Ù† Ù‡Ù†Ø§Ùƒ Ù†Øµ (Ø³ÙˆØ§Ø¡ Ù…Ù† Ø§Ù„ØµÙˆØª Ø£Ùˆ ÙƒØªØ§Ø¨Ø© ÙŠØ¯ÙˆÙŠØ© Ø¥Ø°Ø§ Ø£Ø±Ø¯Øª Ø¥Ø¶Ø§ÙØªÙ‡Ø§ Ù„Ø§Ø­Ù‚Ø§Ù‹)
if user_text:
    with st.spinner("ğŸ¤– Ø§Ù„Ù…Ø³ØªØ´Ø§Ø± ÙŠÙÙƒØ± ÙˆÙŠØ¬Ù‡Ø² Ø§Ù„Ø±Ø¯..."):
        try:
            # 2. Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©
            response = model.generate_content(user_text)
            answer_text = response.text
            
            # Ø¹Ø±Ø¶ Ø§Ù„Ù†Øµ
            st.markdown(f"### ğŸ“˜ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø©:\n{answer_text}")
            
            # 3. ØªØ­ÙˆÙŠÙ„ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ù„ØµÙˆØª
            output_file = "response.mp3"
            asyncio.run(generate_speech(answer_text, output_file))
            
            # ØªØ´ØºÙŠÙ„ Ø§Ù„ØµÙˆØª ØªÙ„Ù‚Ø§Ø¦ÙŠØ§Ù‹
            st.audio(output_file, format='audio/mp3', autoplay=True)
            
        except Exception as e:
            st.error(f"Ø­Ø¯Ø« Ø®Ø·Ø£: {e}")

st.markdown("---")
st.caption("Ù…Ù„Ø§Ø­Ø¸Ø©: ØªØ£ÙƒØ¯ Ù…Ù† Ø§Ù„Ø³Ù…Ø§Ø­ Ù„Ù„Ù…ØªØµÙØ­ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Ø§Ù„Ù…ÙŠÙƒØ±ÙˆÙÙˆÙ† ğŸ¤")
